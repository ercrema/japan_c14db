---
title: "Japanese Radiocarbon Database Project"
author: "E.Crema"
date: "12 April 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
fullMode=FALSE
# Load required R libraries
library(tidyverse)
library(readxl)
library(measurements)
library(sp)
library(NipponMap) 
library(mapdata)
library(maptools)
library(Nippon)
library(here)
```

# General Info

This R markdown file documents the cleaning and translation of the [Database of radiocarbon dates published in Japanese archaeological reports](https://www.rekihaku.ac.jp/up-cgi/login.pl?p=param/esrd/db_param) introduced in the following publication:

[工藤雄一郎・坂本稔・箱﨑真隆2018「遺跡発掘調査報告書放射性炭素年代測定データベース作成の取り組み」『国立歴史民俗博物館研究報告』212：pp.251-266.](https://www.rekihaku.ac.jp/outline/publication/ronbun/ronbun9/index.html)

## Repository Structure

The repository contains R scripts, raw data, and look-up translation tables that enables the creation of the final CSV file `japanc14db_v01(190801).csv`. The table below provides a description of each directory within the repository:

| Directory Name |                                  Description                                  |
|:---------------|:------------------------------------------------------------------------------|
| raw            | Contains raw Excel spreadsheets provided by Y.Kudo                            |
| lookup         | Contains look-up tables for translations                                      |
| reports        | Contains interim reports listing issues and problems to be reported to Y.Kudo |
| utility        | Contains R utility functions and scripts                                      |
| output         | Contains all outputs                                                          |
| log            | Contains R log files for generating specific outputs                          |

## Workflow and Organisation

The main workflow consists of three distinct processes: **consistency checks** (e.g. identify duplicates, incorrect spatial coordinates, etc.); **translation**; and **augmentation** (e.g. recovery of coordinated from addresses, links to _Nabunken_ databases, etc.).

# STEP 1: Reading Data into R and Extracting Relevant Fields

We first read the most up-to-date version of the raw data (currently `c14_raw_v210218.xlsx` corresponding to the original file `★＜公開中＞年代測定データベース学術・関東･東北・北陸・中部・九州・北海道・四国20210218.xlsx`):

```{r}
options(warn=-1) #suppress warning
c14raw=read_xlsx(here('raw','c14_raw_v210218.xlsx'), skip=1) |> as.data.frame(stringsAsFactor=FALSE)
```

and assign new names to the columns

```{r}
## Assign New Columns Names (comments includes the original names)
colnames(c14raw)= c("PrefectureCode", #都道府県コード
                    "Prefecture", #都道府県
                    "SiteName", #遺跡名
                    "SiteLocation", #所在地
                    "SamplingLocation",#サンプル採取地点等
                    "MaterialType",#試料の種類
                    "MaterialCode1",#試料コード1
                    "MaterialCode2",#試料コード2
                    "Period",#時代
                    "Phase",#時期
                    "PeriodCode",#時代コード
                    "LabCode",#試料番号
                    "Method",#β線法/AMS法
                    "CRA",#14C年代
                    "elim1",#
                    "CRAError",#14C年代
                    "CorrectedC14Age",#暦年較正用14C年代
                    "elim2",#
                    "CorrectedC14Error",#暦年較正用14C年代
                    "Delta13C",#δ13CAms
                    "elim3",#
                    "Delta13CError",#
                    "Delta13CIRMS",#δ13C（‰）(IR-MS)
                    "AnalysedBy",#分析者（著者）
                    "Laboratory",#測定機関
                    "PubblicationYear",#刊行年
                    "ReportTitle",#報告タイトル
                    "Page",#ページ
                    "Remarks",#備考
                    "Reference",#報告書名
                    "Publisher",#発行者
                    "Latitude",#緯度
                    "Longitude",#経度
                    "Note1",#(非公開）
                    "Note2",#(非公開）
                    "Note3",#(非公開）
                    "Note4",#(非公開）
                    "Note5")#(非公開）
```

which allows us to eliminate columns that are not required for the final database

```{r}
## Eliminate Unecessary Columns
c14raw = c14raw[,-which(names(c14raw)%in%c("elim1","elim2","elim3","Note1","Note2","Note3","Note4","Note5"))]
```

It is not possible to assign a unique identifier for each site, but it is possible to do this based on reference:

```{r}
c14raw$ReferenceID = as.numeric(as.factor(c14raw$Reference))
```

We also want to remove carriage returns and new lines from the references:

```{r}
c14raw$Reference = gsub("[\r\n]", "", c14raw$Reference)
```

And from site names

```{r}
c14raw$SiteName =  gsub("[\r\n]", "", c14raw$SiteName)
c14raw$SiteName =  gsub(" ", "", fixed=TRUE, c14raw$SiteName) #remove also white-space
```

As well as Phases, Periods, and Address

```{r}
c14raw$Phase = gsub("[\r\n]", "", c14raw$Phase)
c14raw$Period = gsub("[\r\n]", "", c14raw$Period)
c14raw$SiteLocation = gsub("[\r\n]", "", c14raw$SiteLocation)
```

Finally we add a column indicating whether a specific entry should be retained or eliminated for analyses.
```{r}
c14raw$retain=TRUE
```

and create a reference so that original input files can be easily identified
```{r}
c14raw$originalRow = 2:(nrow(c14raw)+1)
```

###CHECK001: LabCode and Unique Identifier 

The field `$LabCode` contains the Laboratory Code of each sample and needs to be unique. First we remove all cases with missing or incomplete Code and store them in the report (file: `./reports/missingLabCodes.csv`).

```{r}
missingLabCodes.index = which(is.na(c14raw$LabCode)|c14raw$LabCode=="不明")
noNumbers.index = grep("^[A-Za-z]+$",c14raw$LabCode) #LabCodes without Numbers
noCharacters.index = which(!grepl("\\D", c14raw$LabCode))#LabCodes with only Numbers
questionMarks.index = grep("\\?", c14raw$LabCode) #LabCodes with questionmarks
labcodeIssues = unique(c(missingLabCodes.index,noNumbers.index,noCharacters.index,questionMarks.index))
c14raw.labcodeissue = c14raw[labcodeIssues,]
c14raw$retain[labcodeIssues] = FALSE 

write.csv(c14raw.labcodeissue,here('reports','missingLabCodes.csv'),row.names=FALSE) 
```

We then check instances of duplicate Codes: 

```{r}
any(duplicated(c14raw$LabCode[-labcodeIssues])) #Check if there are any duplicates even after removing problematic cases
duplicates=sort(table(c14raw$LabCode[-labcodeIssues])[which(table(c14raw$LabCode[-labcodeIssues])>1)],TRUE)
```

The result indicates `r nrow(duplicates)` unique LabCodes that have somewhere between `r min(duplicates)-1` to `r max(duplicates)-1` duplicate entries. This might be partly conditioned by the report "弥生農耕の起源と東アジア-炭素14年代測定による高精度編年体系の構築-平成16～20年文部科学省・科学研究費学術創成研究費" which collated several radiocarbon dates from original site reports. Indeed if we look at the duplicated cases:

```{r}
duplicates.index = which(c14raw$LabCode%in%names(duplicates))
duplicates.labcode.c14raw = c14raw[duplicates.index,]
```

This specific entry appears `r as.numeric(sort(table(duplicates.labcode.c14raw$Reference),TRUE)[1])` times. The question is whether all entries from this report are duplicated:

```{r}
c14raw.rekihakuProj = subset(c14raw,retain==TRUE&Reference=="弥生農耕の起源と東アジア-炭素14年代測定による高精度編年体系の構築-平成16～20年文部科学省・科学研究費学術創成研究費　研究成果報告書")
c14raw.NonRekihakuProj = subset(c14raw,retain==TRUE&Reference!="弥生農耕の起源と東アジア-炭素14年代測定による高精度編年体系の構築-平成16～20年文部科学省・科学研究費学術創成研究費　研究成果報告書")
all(c14raw.rekihakuProj$LabCode%in%c14raw.NonRekihakuProj$LabCode)
```

which does not seem to be the case. Indeed the majority (`r length(which(!c14raw.rekihakuProj$LabCode%in%c14raw.NonRekihakuProj$LabCode))` vs `r length(which(c14raw.rekihakuProj$LabCode%in%c14raw.NonRekihakuProj$LabCode))`) seems to actually not be present amongst the rest of the database.

The report `./reports/duplicateLabCodes.csv' contains all instances of duplicates:

```{r}
for (i in 1:length(duplicates))
{ 
  if (i==1)
  {
    duplicatedLabcodes = subset(c14raw[-labcodeIssues],LabCode==names(duplicates)[i])
  }
  if (i>1)
  {
    duplicatedLabcodes=rbind.data.frame(duplicatedLabcodes,subset(c14raw[-labcodeIssues],LabCode==names(duplicates)[i]))
  }
}
write.csv(duplicatedLabcodes,here('reports','duplicateLabCodes.csv'),row.names=FALSE) 
```

Finally the routine below goes through all cases of duplicates and:
* in case of all matching critical information (CRA, error, sitename, etc) retain the most recent publication as it is likely going to provide more details
* in case of mismatch all entries are not retained

```{r}
for (i in 1:length(duplicates))
{ 
  tmp.index = which(c14raw$LabCode==names(duplicates)[i]) 
  tmp = c14raw[tmp.index,c("PrefectureCode","SiteName","SiteLocation","SamplingLocation","MaterialType","MaterialCode1","MaterialCode2","PeriodCode","LabCode","Method","CRA","CRAError","CorrectedC14Age","CorrectedC14Error","Delta13C","Delta13CError")] 
  tmp = c14raw[tmp.index,c("PrefectureCode","MaterialCode1","MaterialCode2","PeriodCode","LabCode","Method","CRA","CRAError")] 
  
  # If the dates are different, both should be eliminated
  if(nrow(unique(tmp))>1){c14raw$retain[tmp.index]=FALSE}
  
  # If the dates are the same, retain the most recent publication
  if(nrow(unique(tmp))==1)
  {
    c14raw$retain[tmp.index]=FALSE
    c14raw$retain[tmp.index[which.max(c14raw$PubblicationYear[tmp.index])]]=TRUE
  } 
}
```

We then compute our final check:

```{r}
c14raw.check = subset(c14raw,retain==TRUE)
anyDuplicated(c14raw.check$LabCode) #if 0 pass
sum(c14raw$retain)
```


# STEP 2: Handle Spatial Coordinates and Translate Prefecture Names

## Convert Missing Lat-Lon into NA
```{r}
c14raw$Latitude[which(c14raw$Latitude=='不明')]=NA
c14raw$Longitude[which(c14raw$Longitude=='不明')]=NA
c14raw$retain[which(is.na(c14raw$Latitude)|is.na(c14raw$Longitude))]=FALSE
```

## Convert Degree Minute Seconds coordinates into Degree Decimals
```{r}
nonNAs = which(is.na(c14raw$Latitude)|is.na(c14raw$Longitude))
c14raw$Latitude[-nonNAs]=as.numeric(char2dms(paste0(c14raw$Latitude[-nonNAs],"N"),chd='゜',chm="'"))
c14raw$Longitude[-nonNAs]=as.numeric(char2dms(paste0(c14raw$Longitude[-nonNAs],"E"),chd='゜',chm="'"))
c14raw$Latitude=as.numeric(c14raw$Latitude)
c14raw$Longitude=as.numeric(c14raw$Longitude)
```


## Estimate Coordinates Using Site Address
```{r}
source(here('utility','extractCoordinates.R'))
unique.Addresses = unique(c14raw$SiteLocation)
#Commented section below is for the first run
#gmap.coord=extractCoordinates(unique.Addresses)
#colnames(gmap.coord)=c("SiteLocation","Latitude.gmap","Longitude.gmap")
#write.csv(gmap.coord,"./output/gmap.coord.csv",row.names = FALSE)
gmap.coord=read.csv(file=here('output','gmap.coord.csv'),stringsAsFactors = FALSE)

if (sum(!unique.Addresses%in%gmap.coord$SiteLocation)>0)
{
missing.Addresses=unique.Addresses[which(!unique.Addresses%in%gmap.coord$SiteLocation)]
gmap.coord.extra=extractCoordinates(missing.Addresses,api = 'AIzaSyBgQGevFZ4GhdUOoZOWYK8SM_b1b3WRo8I')
colnames(gmap.coord.extra)=c("SiteLocation","Latitude.gmap","Longitude.gmap")
gmap.coord=rbind.data.frame(gmap.coord,gmap.coord.extra)
write.csv(gmap.coord,here('output','gmap.coord.csv'),row.names = FALSE)
}
c14raw=left_join(c14raw,gmap.coord)
```


## Translate Prefecture Names
```{r}
pref.lookup = read.csv("./lookup/prefectures_translations.csv",stringsAsFactors=FALSE)
colnames(pref.lookup) = c("Prefecture","PrefectureNameEn","Region")
c14raw=left_join(c14raw,pref.lookup)
```


###CHECK002: Spatial Coordinates {#check_coordinates}

##### 0,0 and Missing Coordinates

Eliminate sites with missing coordinates and coordinates cannot be recovered from their addresses:

```{r}
missing.coord.index = which(c14raw$Latitude==0 & c14raw$Longitude==0 & is.na(c14raw$Latitude.gmap)&is.na(c14raw$Longitude.gmap))
c14raw$retain[missing.coord.index]=FALSE
write.csv(c14raw[missing.coord.index,],here('reports','nocoordinates_and_noreliableaddress.csv'),row.names=FALSE) 

```

##### Mismatch with Prefectures

Check whether the coordinates fall within the designated prefectures using the utility function `checkPrefecture()`, and store them on designated fields:

```{r}
source(here('utility','checkPrefecture.R'))
original.coord.check = checkPrefecture(lat=c14raw$Latitude,lon=c14raw$Longitude,pref = c14raw$PrefectureNameEn)
gmap.coord.check = checkPrefecture(lat=c14raw$Latitude.gmap,lon=c14raw$Longitude.gmap,pref = c14raw$PrefectureNameEn)
save(original.coord.check,gmap.coord.check,file=here('output','reverseGeocode.RData'))

c14raw$coord.check=original.coord.check
c14raw$gmap.coord.check=gmap.coord.check
```

Determine if there are instances of mismatches. Starting from the provided coordinates:

```{r}
tmp.index=which((is.na(c14raw$coord.check)|c14raw$coord.check==FALSE)&c14raw$Latitude!=0&c14raw$Longitude!=0)
tmp=c14raw[tmp.index,]
tmp=unique(tmp[,c("Prefecture","SiteName","SiteLocation","Latitude","Longitude","coord.check")])
```

Manual check.
```{r}
#東京 石野遺跡 is ok
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[1]))]=TRUE
#神奈川   猿島洞穴遺跡 is ok
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[2]))]=TRUE
# 熊本         荘貝塚 (should be Kagoshima)
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[3]))]=FALSE
# 熊本       鞠智城跡 (coodinate in Hokkaido)
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[4]))]=FALSE
# 北海道      山越2遺跡 is ok
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[5]))]=TRUE
# 北海道     落部１遺跡 (coodinate in Aichi)
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[6]))]=FALSE
# 北海道      山越4遺跡 is ok
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[7]))]=TRUE
# 北海道      山越4遺跡 (coodinate in Shizuoka)
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[8]))]=FALSE
# 北海道      山越4遺跡 (coodinate in Aichi)
c14raw$coord.check[which(c14raw$SiteLocation%in%c(tmp$SiteLocation[8]))]=FALSE
```

Set Retain to FALSE for all cases where manual check was FALSE

```{r}
c14raw$retain[which(!c14raw$coord.check)]=FALSE
```


Now check the Google map coordinates

```{r}
tmp.index=which((is.na(c14raw$gmap.coord.check)|c14raw$gmap.coord.check==FALSE)&!is.na(c14raw$Latitude.gmap)&!is.na(c14raw$Longitude.gmap))
tmp=c14raw[tmp.index,]
tmp=unique(tmp[,c("Prefecture","SiteName","SiteLocation","Latitude","Longitude","gmap.coord.check")])
```

There are `r nrow(tmp)` locations with mismatches, indicating that the original coordinates are probably more reliable. We remove the google coordinates:

```{r}
c14raw <- subset(c14raw, select = -c(gmap.coord.check,Latitude.gmap,Longitude.gmap))
```

##### SiteIDs and Coordinates
The database does not provide unique identifier for sites. Thus several sites with different names might refer to the same site, but equally because coordinates were often derived from addresses, multiple sites with different names might have the same spatial coordinate as well. In some rare cases the same site might have also different coordinates in different publications. 

We first check all cases where the same coordinates are matched to different site names.

```{r}
#Assign a temporary unique ID to Coordinate Pairs
c14raw.sitenames=select(c14raw,Prefecture,SiteName,Latitude,Longitude)
comb <- do.call(paste, c(as.list(c14raw.sitenames[c("Latitude","Longitude")]), sep = "."))
c14raw.sitenames$coordid <- match(comb, unique(comb))

#If no instances of multiple coordinates there should be no duplicates in id
c14raw.sitenames.unique=unique(c14raw.sitenames)
any(duplicated(c14raw.sitenames.unique$id))
```


The results indicate there are no instances where the same set of coordinates are referring to different sites. We then check all cases where the same site name is matched to different coordinates.

```{r}
#Assign a temporary unique ID to Prefecture-SiteName pairs
c14raw.sitenames=select(c14raw,Prefecture,SiteName,Latitude,Longitude)
comb <- do.call(paste, c(as.list(c14raw.sitenames[c("Prefecture","SiteName")]), sep = "."))
c14raw.sitenames$id <- match(comb, unique(comb))

#If no instances of multiple coordinates there should be no duplicates in id
c14raw.sitenames.unique=unique(c14raw.sitenames)
any(duplicated(c14raw.sitenames.unique$id))
tmp.index=which(duplicated(c14raw.sitenames.unique$id)) #which index are duplicates
dup.ids=unique(c14raw.sitenames.unique$id[tmp.index])#what are the temporary ID of those duplicates

for (i in 1:length(dup.ids))
{
  tmp.index=which(c14raw.sitenames$id==dup.ids[i])
  tmp.df=c14raw[tmp.index,]
  tmp.df=select(tmp.df,Prefecture,SiteName,SiteLocation,Latitude,Longitude,ReferenceID,Reference)
  tmp.df$rowindex=NA
  tmp.df=unique(tmp.df)
  tmp.df$duplicateID=i
  tmp.df$idistance=NA
  tmp.df$Latitude.mean=mean(tmp.df$Latitude)
  tmp.df$Longitude.mean=mean(tmp.df$Longitude)
  
  for (j in 1:nrow(tmp.df))
  {
    #the following stores the row numbers of the original DB
    row.index = which(c14raw$SiteName==tmp.df$SiteName[j] &
                        c14raw$SiteLocation==tmp.df$SiteLocation[j] &
                        c14raw$Latitude==tmp.df$Latitude[j] & 
                        c14raw$Longitude==tmp.df$Longitude[j] &
                        c14raw$ReferenceID==tmp.df$ReferenceID[j])
    tmp.df$rowindex[j]=paste(row.index,collapse="|")
  }
  
  c14raw$Latitude.new[row.index]
  if (any(is.na(tmp.df[c("Longitude","Latitude")])))
  {
    idistance=9999
  } else {
  idistance=spDists(x=as.matrix(tmp.df[c("Longitude","Latitude")]),longlat=TRUE)}
  if (max(idistance)>0){ #eliminates cases where the differences is just on Reference ID or address
    tmp.df$idistance=max(idistance)  
    if (i==1) {d=tmp.df}
    if (i>1){d=rbind.data.frame(d,tmp.df)}
  }
}

write.csv(d,here('reports','samesite_differentcoordinates.csv'),row.names=FALSE)

```

Set `$Retain` to FALSE when the coordinate mismatch is above 5km:

```{r}
NotRetainIndex = as.numeric(unlist(strsplit(d$rowindex[which(d$idistance>=5)],"[|]")))
c14raw$retain[NotRetainIndex]=FALSE
```

Update coordinates to mean latitude and longitude when mismatch is below 5km
```{r}
RetainUpdateCoordinateIndex = which(d$idistance<5)
for (i in 1:length(RetainUpdateCoordinateIndex))
{
  tmp=d[RetainUpdateCoordinateIndex[i],]
  row.index=as.numeric(unlist(strsplit(tmp$rowindex,"[|]")))
  c14raw$Latitude[row.index]=tmp$Latitude.mean
  c14raw$Longitude[row.index]=tmp$Longitude.mean
}
```


# STEP 3: Translations

## Calibration
```{r}
c14raw$Method_En=NA
c14raw$Method_En[which(c14raw$Method%in%c("β線法","β線","β線法\n"))]="Beta Counting"
c14raw$Method_En[which(c14raw$Method=="AMS法")]="AMS"
c14raw$Method_En[which(c14raw$Method=="不明")]=NA
```

## MatericalCode1 & MaterialCode2
```{r}
matcode1 = read.csv(here('lookup','materialGeneralCode.csv'))
matcode2 = read.csv(here('lookup','materialDetailsCode.csv'))


c14raw=left_join(c14raw,matcode1,by=c("MaterialCode1"="Code"))
c14raw=left_join(c14raw,matcode2,by=c("MaterialCode2"="Code"))
colnames(c14raw)[which(colnames(c14raw)=="Description.x")]="Material"
colnames(c14raw)[which(colnames(c14raw)=="Description.y")]="Material_Details" 
```

## Periods

```{r}
# read translation lookup file, originally created with the following command:
# periods = unique(data.frame(Period=c14db$Period,PeriodEN=NA))
# periods = arrange(periods,Period)
# write.csv(periods,file="./lookup/periods_toTranslate.csv",row.names=F)
periods = unique(read.csv(here('lookup','periods_translation.csv')))

# The following step updates the lookup table
if (any(!unique(as.character(c14raw$Period))%in%periods$Period))
{
period.tmp=data.frame(Period=unique(as.character(c14raw$Period)),stringsAsFactors = FALSE)
period.tmp=left_join(period.tmp,periods,by="Period")
write.csv(period.tmp,file="./lookup/periods_translation_todo.csv",row.names=F)
}
# Before joining this to the main DB
c14raw=left_join(x=c14raw,y=periods,by="Period")
```

## Phases
```{r}
# read translation lookup file, originally created with the following command:
# phases = unique(data.frame(Phase=c14db$Phase,PhaseEN=NA))
# phases = arrange(phases,Phase)
# write.csv(phases,file="./lookup/phases_toTranslate.csv",row.names=F)
phases = read.csv(here('lookup','phases_translation.csv'))


if (any(!unique(as.character(c14raw$Phase))%in%phases$Phase))
{
# The following step updates the lookup table
phases.tmp=data.frame(Phase=unique(as.character(c14raw$Phase)),stringsAsFactors = FALSE)
phases.tmp=left_join(phases.tmp,phases,by="Phase")
write.csv(phases.tmp,file="./lookup/phases_translation_todo.csv",row.names=F)
}

# Before joining this to the main DB
c14raw=left_join(x=c14raw,y=phases,by="Phase")
```

## Material Details [In Progress]
```{r}
# read translation lookup file, originally created with the following command:
# material_details = unique(data.frame(Material_Classification=c14db$Material_Details,Material_Details_JPN=c14db$MaterialType,Material_Details_EN=NA))
# material_details = arrange(material_details,Material_Details_JPN)
# # In some cases the same `Material_Details_JPN` (`MaterialType` in c14db) was assigned to different `Material_Classification` (`MaterialCode2` in c14db)
# inconsistent_materials = names(which(table(material_details$Material_Details_JPN)>1))
# material_details$inconsistent=FALSE
# material_details$inconsistent[which(material_details$Material_Details_JPN%in%inconsistent_materials)]=TRUE
# write.csv(material_details,file="./lookup/material_details.csv",row.names=F)
material_details <- read.csv(here('lookup','material_details.csv'))

# Final Column Names need to be Defined


```

## Period Codes

```{r}
# periodcode = unique(data.frame(Code=c14db$PeriodCode,PeriodCodeEn=NA))
# periodcode = arrange(periodcode,Code)
# write.csv(periodcode,file="./lookup/periodcode.csv",row.names=F)
periodcode = read.csv("./lookup/periodcode.csv")
periodcode = read.csv(here('lookup','periodcode.csv'))

if (any(!unique(as.character(c14raw$PeriodCode))%in%periodcode$Code))
{
# The following step updates the lookup table
code.tmp=data.frame(Code=unique(as.character(c14raw$PeriodCode)),stringsAsFactors = FALSE)
periodcode.tmp=left_join(code.tmp,periodcode,by="Code")
write.csv(periodcode.tmp,file="./lookup/periodcode_to_do.csv",row.names=F)
}

c14raw=left_join(x=c14raw,y=periodcode,by=c("PeriodCode"="Code"))
```



## Site Names

Site names have been romanised by first using the Nabunken Site Report Database to automatically extract the _furigana_ of site names in _kanji_, followed by a manual check for fixing potential mismatch and missing _furigana_. Romanisation has been carried out using the `kana2roma()` function from the `Nippon` package. 
```{r}
library(Nippon)
source(here('utility','siteNameTranscript.R'))
### The following block is executed only in the very first iteration:
#siteList = unique(c14raw$SiteName)
#sitenames=siteNameTranscript(siteList)
##Export for manual editing
#sitenames$matched=(sitenames$SiteName==sitenames$MatchedName)
#sitenames$ToCheck = !sitenames$matched | is.na(sitenames$Furigana)
#sitenames=dplyr::select(sitenames,SiteName,Furigana,ToCheck)
#sitenames$Alternative=NA
#write.csv(sitenames,"./lookup/sitenamesTranscript.csv",row.names = FALSE)

###This block is from second iteration onwards
#Import manually edited file
sitenames=read.csv(here('lookup','sitenamesTranscript.csv'),stringsAsFactors = FALSE)

siteList = unique(c14raw$SiteName)

# The following step updates the lookup table in case there are some missing sites
if (!all(unique(siteList)%in%sitenames$SiteName))
{
  missingSites = unique(siteList)[which(!unique(siteList)%in%sitenames$SiteName)]
  additional_sitenames=siteNameTranscript(missingSites)
  sitenames=rbind.data.frame(sitenames,data.frame(SiteName=additional_sitenames$SiteName,
                                                      Furigana=additional_sitenames$Furigana,
                                                      ToCheck=TRUE,
                                                      Alternative=NA))
  write.csv(sitenames,here('lookup','sitenamesTranscript.csv'),row.names = FALSE)
}

# The section reads and merges the sitenames to the main data.frame
sitenames=read.csv(here('lookup','sitenamesTranscript.csv'),stringsAsFactors = FALSE,na.strings = '')

### Processing Site Type ####
sitenames$SiteType=NA
# 遺跡群
sitenames$SiteType[grep('いせきぐん',sitenames$Furigana)]='Site Cluster'
# 遺跡
sitenames$SiteType[which(is.na(sitenames$SiteType)&grepl('いせき',sitenames$Furigana))]='Site'
sitenames$SiteType[which(is.na(sitenames$SiteType)&grepl('遺跡',sitenames$SiteName))]='Site'

# 貝塚
sitenames$SiteType[grep('かいづか',sitenames$Furigana)]='Shell Midden'
sitenames$SiteType[grep('貝塚',sitenames$SiteName)]='Shell Midden'

# 窯跡群
sitenames$SiteType[grep('窯跡群',sitenames$SiteName)]='Kiln Cluster'
# 窯跡
sitenames$SiteType[which(is.na(sitenames$SiteType)&grepl('窯跡',sitenames$SiteName))]='Kiln'
sitenames$SiteType[grep('窯跡群',sitenames$SiteName)]='Kiln'
sitenames$SiteType[grep('窯',sitenames$SiteName)]='Kiln'

# 館跡
sitenames$SiteType[grep('館跡',sitenames$SiteName)]='Fort'
# 城跡
sitenames$SiteType[grep('城跡',sitenames$SiteName)]='Castle'
# 古墳
sitenames$SiteType[grep('古墳',sitenames$SiteName)]='Kofun'
# 洞窟
sitenames$SiteType[grep('洞窟',sitenames$SiteName)]='Cave'
sitenames$SiteType[grep('洞穴',sitenames$SiteName)]='Cave'
# 城
sitenames$SiteType[grep('城',sitenames$SiteName)]='Castle'
# 横穴
sitenames$SiteType[grep('横穴',sitenames$SiteName)]='Side-Hole Burial'
# 水田址
sitenames$SiteType[grep('水田址',sitenames$SiteName)]='Paddy Field'
# 寺跡
sitenames$SiteType[grep('寺跡',sitenames$SiteName)]='Temple/Shrine'
# 列石
sitenames$SiteType[grep('列石',sitenames$SiteName)]='Standing Stones'
# Everything Else
sitenames$SiteType[which(is.na(sitenames$SiteType))]='Other'

# Romanise ####
sitenames$Romanised = kana2roma(sitenames$Furigana)
sitenames$Romanised = stringr::str_to_title(sitenames$Romanised)
#Merge Back to Main DB
c14raw=left_join(x=c14raw,y=sitenames,by="SiteName")


# Report any case of sitenames with suggested correction
if (any(!is.na(c14raw$Alternative)))
{
 fix.names=subset(c14raw,!is.na(Alternative))
 write.csv(fix.names,file=here('reports','sitenames_issues.csv'))

}



```


## Assign retain to FALSE for impossible dates and NA dates
```{r}
c14raw$retain[which(c14raw$CRA>55000|c14raw$CRA<0)]=FALSE
c14raw$retain[which(is.na(c14raw$CRA)|is.na(c14raw$CRAError))]=FALSE
c14raw$retain[which(c14raw$CRAError>500)]=FALSE

```

## Assign retain to FALSE for LabCodes with '不明' or ?
```{r}
c14raw$retain[c14raw$LabCode=='不明']=FALSE
c14raw$retain[grep('\\？',c14raw$LabCode)]=FALSE
c14raw$retain[grep('\\?',c14raw$LabCode)]=FALSE
```

## Eliminate Any Duplicates
```{r}
# All Duplicates
c14raw = unique(c14raw)
# Duplicated LabCode
tmp = subset(c14raw,retain==TRUE)
dupllab=names(table(tmp$LabCode))[which(table(tmp$LabCode)>1)]
## Manual Fix ###
c14raw[which(c14raw$LabCode%in%c(dupllab))[c(1,3,5)],'retain']=FALSE
```


## Store Output 
```{r}
write.csv(c14raw,file='japanc14db_v03.1(210218).csv')
c14db=c14raw
save(c14db,file='japanc14db_v03.1(210218).RData')
```


<!-- This section is experimental and not working that well
## STEP 4: Link to Nabunken URL

The `refNabunkenLink()` scrapes and attempts to match references to the Nabunken Site Report Database, returning potential matching URLs and DOIs. These have been manually checked before being joined to the main database. 

```{r,eval=fullMode,echo=fullMode}
source("./utility/nabunkenLink.R")
### The following block is executed only in the very first iteration:
# c14raw.tmp = unique(data.frame(site=c14raw$SiteName,reference=c14raw$Reference))
# nabunkenRes=refNabunkenLink(site=c14raw.tmp$site,reference=c14raw.tmp$reference)
# nabunkenRes$toCheck=TRUE
# write.csv(nabunkenRes,"./lookup/nabunkenURL_linker.csv",row.names = FALSE)

###This block is from second iteration onwards
#Import manually edited file
nabunkenRes=read.csv("./lookup/nabunkenURL_linker.csv",stringsAsFactors = FALSE)
#check if there are any new references TO FINISH
c14raw.tmp = unique(data.frame(site=c14raw$SiteName,reference=c14raw$Reference,stringsAsFactors = FALSE))

if (any(!c14raw.tmp$reference%in%nabunkenRes$reference))
{
  i=which((!c14raw.tmp$reference%in%nabunkenRes$reference))
  nabunkenRes2add=refNabunkenLink(site=c14raw.tmp$site[i],reference=c14raw.tmp$reference[i])
  nabunkenRes2add$toCheck=TRUE
  nabunkenRes=rbind.data.frame(nabunkenRes,nabunkenRes2add)
}
write.csv(nabunkenRes,"./lookup/nabunkenURL_linker.csv",row.names = FALSE)


nabunkenRes=read.csv("./lookup/nabunkenURL_linker.csv",stringsAsFactors = FALSE)
c14raw=left_join(x=c14raw,y=nabunkenRes,by=c("Reference"="reference"))
```

```{r,eval=!fullMode,echo=!fullMode}
source("./utility/nabunkenLink.R")
### The following block is executed only in the very first iteration:
# c14raw.tmp = unique(data.frame(site=c14raw$SiteName,reference=c14raw$Reference))
# nabunkenRes=refNabunkenLink(site=c14raw.tmp$site,reference=c14raw.tmp$reference)
# nabunkenRes$toCheck=TRUE
# write.csv(nabunkenRes,"./lookup/nabunkenURL_linker.csv",row.names = FALSE)

###This block is from second iteration onwards
# #Import manually edited file
# nabunkenRes=read.csv("./lookup/nabunkenURL_linker.csv",stringsAsFactors = FALSE)
# #check if there are any new references TO FINISH
# c14raw.tmp = unique(data.frame(site=c14raw$SiteName,reference=c14raw$Reference,stringsAsFactors = FALSE))
# 
# if (any(!c14raw.tmp$reference%in%nabunkenRes$reference))
# {
#   i=which((!c14raw.tmp$reference%in%nabunkenRes$reference))
#   nabunkenRes2add=refNabunkenLink(site=c14raw.tmp$site[i],reference=c14raw.tmp$reference[i])
#   nabunkenRes2add$toCheck=TRUE
#   nabunkenRes=rbind.data.frame(nabunkenRes,nabunkenRes2add)
# }
# write.csv(nabunkenRes,"./lookup/nabunkenURL_linker.csv",row.names = FALSE)


nabunkenRes=read.csv("./lookup/nabunkenURL_linker.csv",stringsAsFactors = FALSE)
c14raw=left_join(x=c14raw,y=nabunkenRes,by=c("Reference"="reference"))
```

-->







