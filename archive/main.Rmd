---
title: "Japanese Radiocarbon Database Project"
author: "E.Crema"
date: "15 March 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load required R libraries
library(tidyverse)
library(readxl)
library(measurements)
library(sp)
library(NipponMap) 
library(mapdata)
library(maptools)
library(Nippon)
library(here)
```

# General Info

This R markdown file documents the cleaning and translation of the [Database of radiocarbon dates published in Japanese archaeological reports](https://www.rekihaku.ac.jp/up-cgi/login.pl?p=param/esrd/db_param) introduced in the following publication:

[工藤雄一郎・坂本稔・箱﨑真隆2018「遺跡発掘調査報告書放射性炭素年代測定データベース作成の取り組み」『国立歴史民俗博物館研究報告』212：pp.251-266.](https://www.rekihaku.ac.jp/outline/publication/ronbun/ronbun9/index.html)

## Repository Structure

The repository contains R scripts, raw data, and look-up translation tables that enables the creation of the final CSV file `japanc14db_v04(210715).csv`. The table below provides a description of each directory within the repository:

| Directory Name |                                  Description                                  |
|:---------------|:------------------------------------------------------------------------------|
| raw            | Contains raw Excel spreadsheets provided by Y.Kudo                            |
| lookup         | Contains look-up tables for translations                                      |
| reports        | Contains interim reports listing issues and problems to be reported to Y.Kudo |
| utility        | Contains R utility functions and scripts                                      |
| output         | Contains all outputs                                                          |
| log            | Contains R log files for generating specific outputs                          |

## Workflow and Organisation

The main workflow consists of three distinct processes: **consistency checks** (e.g. identify duplicates, incorrect spatial coordinates, etc.); **translation**; and **augmentation** (e.g. recovery of coordinated from addresses, links to _Nabunken_ databases, etc.).

# STEP 1: Reading Data into R and Extracting Relevant Fields

We first read the most up-to-date version of the raw data (currently `c14_raw_v220303.csv` corresponding to the original file `★＜公開中＞年代測定データベース20220302.xlsx`, without the first row):

```{r}
options(warn=-1) #suppress warning
c14raw=read.csv(here('raw','c14_raw_v220303.csv'),na.strings = '',skip=2)
```

and assign column names

```{r}
## Assign New Columns Names (comments includes the original names)
colnames(c14raw)= c("PrefectureCode", #都道府県コード
                    "Prefecture", #都道府県
                    "SiteName", #遺跡名
                    "SiteLocation", #所在地
                    "SamplingLocation",#サンプル採取地点等
                    "MaterialType",#試料の種類
                    "MaterialCode1",#試料コード1
                    "MaterialCode2",#試料コード2
                    "Period",#時代
                    "Phase",#時期
                    "PeriodCode",#時代コード
                    "LabCode",#試料番号
                    "Method",#β線法/AMS法
                    "CRA",#14C年代
                    "elim1",#
                    "CRAError",#14C年代
                    "UnroundedCRA",#暦年較正用14C年代
                    "elim2",#
                    "UnroundedCRAError",#暦年較正用14C年代
                    "Delta13C",#δ13CAms
                    "elim3",#
                    "Delta13CError",#
                    "Delta13CIRMS",#δ13C（‰）(IR-MS)
                    "AnalysedBy",#分析者（著者）
                    "Laboratory",#測定機関
                    "PubblicationYear",#刊行年
                    "ReportTitle",#報告タイトル
                    "Page",#ページ
                    "Remarks",#備考
                    "Reference",#報告書名
                    "Publisher",#発行者
                    "Latitude",#緯度
                    "Longitude",#経度
                    "Note1",#(非公開）
                    "Note2",#(非公開）
                    "Note3",#(非公開）
                    "Note4",#(非公開）
                    "Note5")#(非公開）
```

which allows us to eliminate columns that are not required for the final database

```{r}
## Eliminate Unnecessary Columns
c14raw = c14raw[,-which(names(c14raw)%in%c("elim1","elim2","elim3","Note1","Note2","Note3","Note4","Note5"))]
```

It is not possible to assign a unique identifier for each site, but it is possible to do this based on reference:

```{r}
c14raw$ReferenceID = as.numeric(as.factor(c14raw$Reference))
```

Extract nabunken URL from Remarks and add to dedicated field:

```{r}
c14raw$nabunkenURL = as.character(str_match_all(c14raw$Remarks, 'https://sitereports.nabunken.go.jp/\\d+'))
c14raw$nabunkenURL[which(c14raw$nabunkenURL=='character(0)')]=NA
```
#### String format pre-processing

We want to remove carriage return, new lines, and white spaces from Site names and references:

```{r}
c14raw$Reference = gsub("[\r\n]", "", c14raw$Reference)
c14raw$SiteName =  gsub("[\r\n]", "", c14raw$SiteName)
c14raw$SiteName =  gsub(" ", "", fixed=TRUE, c14raw$SiteName)
c14raw$Phase = gsub("[\r\n]", "", c14raw$Phase)
c14raw$Period = gsub("[\r\n]", "", c14raw$Period)
c14raw$SiteLocation = gsub("[\r\n]", "", c14raw$SiteLocation)
```


#### Issues field and cross-reference to original

The final database should:
  * exclude all instances of duplicated dates.
  * exclude all intsnaces of other major inconsistencies.
  * record latitude and longitude as NA for instances when geographic coordinates are highly innacurate coordinates (i.e. an estimated error above 1km or 0.01 decimal degree)

The boolean fields `duplicate`, `innaccurate_coordinate`, and `other_issues` handle these three cases. When `duplicate` is equal to TRUE the sample will be excluded from the final database, when `innaccurate_coordinate` is TRUE the lat/lon information will be updated to NA. When `other_issues` is set to TRUE the sample will also be eliminated. In all cases the defaul setting is FALSE.

```{r}
c14raw$duplicate = FALSE
c14raw$innacurate_coordinate = FALSE
c14raw$other_issues =FALSE
c14raw$other_issues_notes = ""
c14raw$non_crucial_issues_notes = ""
```

We use the row number as a foreign key to link back to the original input files. 

```{r}
c14raw$originalRow = 2:(nrow(c14raw)+1)
```

*NOTE: This should ideally be based on a unique identifier in the final version of the database*


###CHECK001: C14 Age fields

Several dates have non-numeric characters. These are mostly tailspaces:

```{r}
c14raw$UnroundedCRA = trimws(c14raw$UnroundedCRA)
c14raw$UnroundedCRAError = trimws(c14raw$UnroundedCRAError)
c14raw$CRA = trimws(c14raw$CRA)
c14raw$CRAError = trimws(c14raw$CRAError)
```

But there are some instances of non-numeric fields as well.

```{r}
c14raw$UnroundedCRA[grep('[^0-9.-]',c14raw$UnroundedCRA)]
c14raw$UnroundedCRAError[grep('[^0-9.-]',c14raw$UnroundedCRAError)]
c14raw$CRA[grep('[^0-9.-]',c14raw$CRA)]
c14raw$CRAError[grep('[^0-9.-]',c14raw$CRAError)]
```

A good proportion of these are coded as "Modern" or alternatively stored with non-numeric symbols (e.g. ">35971"). 

```{r}
index = unique(c(grep('[^0-9.-]',c14raw$CRA),
                 grep('[^0-9.-]',c14raw$CRAError)))
c14raw[index,c('CRA','CRAError')]
c14raw$other_issues[index] = TRUE
c14raw$other_issues_notes[index] = 'Non numeric values in date field'
```

###CHECK002: LabCode and Unique Identifier 

The field `$LabCode` contains the Laboratory Code of each sample and needs to be unique. First we note all cases with missing or incomplete Code.

```{r}
index.na = which(is.na(c14raw$LabCode)|c14raw$LabCode=="不明")
index.only.letters = which(!grepl("[^A-Za-z]", c14raw$LabCode))
index.only.numbers = which(!grepl("\\D", c14raw$LabCode))
index.questionmark = grep("\\?", c14raw$LabCode) #LabCodes with questionmarks

c14raw$non_crucial_issues_notes[index.only.letters] = "Labcode does not contain numbers"
c14raw$non_crucial_issues_notes[index.only.numbers] = "Labcode does not contain letters"
c14raw$non_crucial_issues_notes[index.questionmark] = "Labcode contains a question mark"
c14raw$non_crucial_issues_notes[index.na] = "Missing LabCode"
```

We then check instances of duplicated codes. 

```{r}
any(duplicated(c14raw$LabCode))
duplicates=sort(table(c14raw$LabCode)[which(table(c14raw$LabCode)>1)],TRUE)
```

These can be either different dates having the same incorrect LabCode (as the ones identified above), or genuine duplicated dates. The latter is in part caused by the report "弥生農耕の起源と東アジア-炭素14年代測定による高精度編年体系の構築-平成16～20年文部科学省・科学研究費学術創成研究費" which collated several radiocarbon dates from original site reports.

There is a total of `r length(duplicates)` LabCodes that appear in the database. The following script handles them as follows:

1. If the labcode is NA or missing, then consider date to be unreliable
2. If the labcode is problematic (i.e. character or numbers only), then check whether other key info have also any duplicates. If there are none do nothing. If there are duplicates consider all instances to be unreliable.
3. If the labcode is non-problematic check if the oher key info have duplicates. If these info are different, take not that the labcode is incorrect. If these are the same select he oldest reference and/or the one that is not from the rekihaku project mentioned above. 

<!-- from here -->
```{r}
keycolumns = c("PrefectureCode","SiteName","MaterialCode1","MaterialCode2","LabCode","Method","CRA","CRAError")
manual_checks = numeric()
c14raw$other_issues[which(is.na(c14raw$LabCode))] = TRUE
c14raw$other_issues_notes[which(is.na(c14raw$LabCode))] = 'Labcode missing'


for (i in 1:length(duplicates))
{
  labcodename = names(duplicates[i])
  index.labcodename = which(c14raw$LabCode==labcodename)
  number.cases = as.numeric(duplicates[i])
  
  if(labcodename=="不明") {
	  c14raw$other_issues[index.labcodename] = TRUE
	  c14raw$other_issues_notes[index.labcodename] = 'Labcode missing'
  }

  # Case 2: 
  tmp = unique(c14raw[index.labcodename,keycolumns])
  tmp.number.cases = nrow(tmp)

  if (number.cases == tmp.number.cases) # if labcode is the same but the dates info are different
  {
	  c14raw$other_issues_notes[index.labcodename] = 'Same labcode used for different dates'
	  c14raw$other_issues[index.labcodename] = TRUE
  }

  if (number.cases != tmp.number.cases) # if labcode and other info are shared
  {
	  if (tmp.number.cases==1)  # if all duplicates, select entry with the earliest publication date:
	  {
		  pubyears = c14raw$PubblicationYear[index.labcodename]
		  min.pubyear.index = which(pubyears==min(pubyears))
		  if (length(min.pubyear.index)>1) 
			  {
				  c14raw$non_crucial_issues_notes[index.labcodename] = 'Duplicates with the same publication year'
				  min.pubyear.index = sample(min.pubyear.index,size=1)
			  }
		  if (length(min.pubyear.index)==1)
		  {
			  index.duplicates = index.labcodename[which(pubyears!=min(pubyears))]
			  c14raw$duplicate[index.duplicates] = TRUE
		  }
	  }
	  if (tmp.number.cases>1)
	  {
		  manual_checks = c(manual_checks,i)
		  c14raw$other_issues[index.labcodename] =TRUE
		  c14raw$other_issues_notes[index.labcodename] = 'Multiple inconsistent duplicates / Same labcode different dates'
	  }
  }
}

# Notes
# Recheck i =c(72,73,74,75,76,77)
```

Manual checking have identified that several duplicates are from 岩屋遺跡 which is also recorded as 岩屋遺跡F地区. Thera a three copies of each date. The first set is from the rekihaku project, the second and the third set are identical entries due to the fact that dates were presented twice in the same report. In this case we arbitrary select one of the two sets.

```{r}
manual.fix.labcode.id = which(c14raw$LabCode%in%names(duplicates[c(72:77)])) 
c14raw$duplicate[manual.fix.labcode.id[1:12]] = NA
c14raw$other_issues_notes[manual.fix.labcode.id[1:12] = ""
c14raw$non_crucial_issues_notes[manual.fix.labcode.id[12]] = ""
```


###CHECK003: Spatial Coordinates

Initial setup to record problematic cases

```{r}
c14raw$coordinate_issues = FALSE
c14raw$coordinate_issues_notes = "" 
```

First we identify instances of missing coordinates and convert them as NA:

```{r}
c14raw$Latitude[which(c14raw$Latitude=='不明')]=NA
c14raw$Longitude[which(c14raw$Longitude=='不明')]=NA
coordinate.na.index = which(is.na(c14raw$Latitude)|is.na(c14raw$Longitude))
c14raw$coordinate_issues[coordinate.na.index] = TRUE
c14raw$coordinate_issues_notes[coordinate.na.index] = 'Missing coordinates'
```

We then convert degree minute seconds into degree decimals:

```{r}
nonNAs = which(is.na(c14raw$Latitude)|is.na(c14raw$Longitude))
c14raw$Latitude[-nonNAs]=as.numeric(char2dms(paste0(c14raw$Latitude[-nonNAs],"N"),chd='゜',chm="'"))
c14raw$Longitude[-nonNAs]=as.numeric(char2dms(paste0(c14raw$Longitude[-nonNAs],"E"),chd='゜',chm="'"))
c14raw$Latitude=as.numeric(c14raw$Latitude)
c14raw$Longitude=as.numeric(c14raw$Longitude)
```

We then translate prefecture names:

```{r}
# Remove '-ken' and '-fu'
c14raw$Prefecture = str_remove(c14raw$Prefecture,'府')
c14raw$Prefecture = str_remove(c14raw$Prefecture,'県')
# Match to Lookup Table
pref.lookup = read.csv("./lookup/prefectures_translations.csv",stringsAsFactors=FALSE)
colnames(pref.lookup) = c("Prefecture","PrefectureNameEn","Region")
c14raw=left_join(c14raw,pref.lookup)
```

Check whether the coordinates fall within the designated prefectures using the utility function `checkPrefecture()`, and store them on designated fields:

```{r}
source(here('utility','checkPrefecture.R'))
original.coord.check = checkPrefecture(lat=c14raw$Latitude,lon=c14raw$Longitude,pref = c14raw$PrefectureNameEn)
c14raw$coord.prefecture.check=original.coord.check
```
Determine if there are instances of mismatches between coordinates and prefectures:

```{r}
manual.prefecture.check.index=which((c14raw$coord.prefecture.check==FALSE|is.na(c14raw$coord.prefecture.check))&!is.na(c14raw$Latitude))
manual.prefecture.check = c14raw[manual.prefecture.check.index,]
manual.prefecture.check = unique(manual.prefecture.check[,c("Prefecture","SiteName","SiteLocation","Latitude","Longitude","coord.prefecture.check")])
```

Manual check.
```{r}
#東京 石野遺跡 is ok
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[1]))]=TRUE
#神奈川   猿島洞穴遺跡 is ok
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[2]))]=TRUE
# 熊本         荘貝塚 (should be Kagoshima)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[3]))]=FALSE
# 熊本       鞠智城跡 (coodinate in Hokkaido)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[4]))]=FALSE
# 北海道      山越2遺跡 is ok
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[5]))]=TRUE
# 北海道     落部１遺跡 (coodinate in Aichi)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[6]))]=FALSE
# 北海道      山越4遺跡 is ok
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[7]))]=TRUE
# 北海道      倉知川右岸遺跡 (coodinate in Shizuoka)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[8]))]=FALSE
# 徳島       大谷尻遺跡 (coodinate in Aichi) ... correct coordinate is  34.0566 133.9768
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[9]))]=FALSE
# 奈良       観音寺本馬遺跡 (coordinate in Yamagata)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[10]))]=FALSE
# 鳥取  米子城跡(第33次調査) (coordinate in Tottori) ... correct coordinate is 35.424722 133.329166 (small discrepancy)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[11]))]=TRUE
# 島根  湯里天神遺跡 coordinate in Gunma ... correct coordinate is 35.105833 132.380277 (large discrepancy)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[12]))]=FALSE
# 鳥取  久見高丸遺跡 coordinate in Tottori ... correct coordinate is 36.330555 133.238888 (small discrepancy)
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[13]))]=TRUE
# 新潟  堂屋敷遺跡 coordinate in Saitama 
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[14]))]=FALSE
# 長野  東峰遺跡 coordinate in Gunma
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[15]))]=FALSE
# 長野  旭久保C遺跡 coordinate in Gunma
c14raw$coord.prefecture.check[which(c14raw$SiteLocation%in%c(manual.prefecture.check$SiteLocation[16]))]=FALSE
```

Record problematic cases:
```{r}
index.non.matching.pref = which(c14raw$coord.prefecture.check==FALSE)
c14raw$coordinate_issues[index.non.matching.pref] = TRUE
c14raw$coordinate_issues_notes[index.non.matching.pref] = 'Inaccurate coordinate not matching prefecture'
```

The database does not provide unique identifier for sites. Thus several sites with different names might refer to the same site, but equally because coordinates were often derived from addresses, multiple sites with different names might have the same spatial coordinate as well. In some rare cases the same site might have also different coordinates in different publications. 

We first check all cases where the same coordinates are matched to different site names.

```{r}
#Assign a temporary unique ID to Coordinate Pairs
c14raw.sitenames=select(c14raw,Prefecture,SiteName,Latitude,Longitude)
comb <- do.call(paste, c(as.list(c14raw.sitenames[c("Latitude","Longitude")]), sep = "."))
c14raw.sitenames$coordid <- match(comb, unique(comb))

#If no instances of multiple coordinates there should be no duplicates in id
c14raw.sitenames.unique=unique(c14raw.sitenames)
any(duplicated(c14raw.sitenames.unique$id))
```

The results indicate there are no instances where the same set of coordinates are referring to different sites. We then check all cases where the same site name is matched to different coordinates.

```{r}
#Assign a temporary unique ID to Prefecture-SiteName pairs
c14raw.sitenames=select(c14raw,Prefecture,SiteName,Latitude,Longitude)
comb <- do.call(paste, c(as.list(c14raw.sitenames[c("Prefecture","SiteName")]), sep = "."))
c14raw.sitenames$id <- match(comb, unique(comb))

#If no instances of multiple coordinates there should be no duplicates in id
c14raw.sitenames.unique=unique(c14raw.sitenames)
any(duplicated(c14raw.sitenames.unique$id))
```
Here the result is TRUE, indicating that we do have cases where the same site name is matched to different coordinates. 

```{r}
index.same.site.diff.coord = which(duplicated(c14raw.sitenames.unique$id)) #which index are duplicates
dup.ids = unique(c14raw.sitenames.unique$id[index.same.site.diff.coord])#what are the temporary ID of those duplicates

for (i in 1:length(dup.ids))
{
	tmp.index=which(c14raw.sitenames$id==dup.ids[i])
	tmp.df=c14raw[tmp.index,]
	tmp.df=select(tmp.df,PrefectureNameEn,SiteName,Latitude,Longitude) |> unique()

	if(all(is.na(tmp.df$SiteName)))
	{
		next()
	}


	if (length(unique(tmp.df$PrefectureNameEn))==1)
	{

		c14raw$coordinate_issues[tmp.index] = TRUE
		tmp.df.sites  <-  tmp.df
		coordinates(tmp.df.sites)  <- c("Longitude","Latitude")
		proj4string(tmp.df.sites)  <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
		tmp.dist_mat  <- spDists(tmp.df.sites,longlat=TRUE) #inter-site distance matrix
		c14raw$coordinate_issues_notes[tmp.index] = paste0("Same site name with different coordinates (dist=",round(max(tmp.dist_mat),2)," km)")
	}

	if (length(unique(tmp.df$PrefectureNameEn))>1)
	{
		print(i)
		break()
	}
}
```

### C14ages

The field `CRA` should contain rounded 14C ages after corrections for isotopic fractionation, whilst the field `UnroundedCRA` contains the same date without rounding. In theory the difference between the two dates should be small, but the list below suggests larger discrepancies suggestive of input error


```{r}
threshold = 50
absDiff = abs(as.numeric(c14raw$CRA)-as.numeric(c14raw$UnroundedCRA))
index.above.threshold = which(absDiff>threshold)
c14raw$other_issues[index.above.threshold] = TRUE
c14raw$other_issues_notes[index.above.threshold] = paste0(c14raw$other_issues_notes[index.above.threshold],"| difference between unrounded and CRA >",threshold) 
```
<!-- from here -->

# Translations

## Calibration
```{r}
c14raw$Method_En=NA
c14raw$Method_En[which(c14raw$Method%in%c("β線法","β線","β線法\n"))]="Beta Counting"
c14raw$Method_En[which(c14raw$Method=="AMS法")]="AMS"
c14raw$Method_En[which(c14raw$Method=="不明")]=NA
```

## MatericalCode1 & MaterialCode2
```{r}
matcode1 = read.csv(here('lookup','materialGeneralCode.csv'))
matcode2 = read.csv(here('lookup','materialDetailsCode.csv'))


c14raw=left_join(c14raw,matcode1,by=c("MaterialCode1"="Code"))
c14raw=left_join(c14raw,matcode2,by=c("MaterialCode2"="Code"))
colnames(c14raw)[which(colnames(c14raw)=="Description.x")]="Material"
colnames(c14raw)[which(colnames(c14raw)=="Description.y")]="Material_Details" 
```

## Material Details

Material detail is translated by using a lookup table. First we clean the relevant field

```{r}
c14raw$MaterialType = gsub("[\r\n]", "", c14raw$MaterialType) #Remove escapes
c14raw$MaterialType = gsub("　", " ", c14raw$MaterialType) #Convert to half-width encoding
c14raw$MaterialType = gsub("(^[[:space:]]+|[[:space:]]+$)", "", c14raw$MaterialType) #remove trail space
```
We then create a data.frame with unique instances of `MaterialType`:
<!-- From here -->
```{r}
material_details_lookup=read.csv(here('lookup','Material_details_2021_08_17_CJS.csv')) 
material_details_lookup=select(material_details_lookup,Material_Details_JPN,Materials_Details_EN) |> unique()
material_columns = select(c14raw,MaterialType)
material_combined = left_join(material_columns,material_details_lookup,by = c('MaterialType'='Material_Details_JPN')) |> unique()
material_combined$Notes = ''
material_combined$Notes[is.na(material_combined$Materials_Details_EN)] = 'Needs translation'
material_combined$Notes[which(material_combined$MaterialType %in%  names(which(table(material_combined$MaterialType)>1)))] = "Multiple translations"
material_combined = material_combined[order(material_combined$MaterialType),]
write.csv(material_combined,"materials_lookuptable_march2022.csv",na = "",row.names=FALSE)
```

<!-- Need to add merge back to database above -->

## Period Codes

```{r}
# periodcode = unique(data.frame(Code=c14db$PeriodCode,PeriodCodeEn=NA))
# periodcode = arrange(periodcode,Code)
# write.csv(periodcode,file="./lookup/periodcode.csv",row.names=F)
periodcode = read.csv(here('lookup','periodcode.csv'))

if (any(!unique(as.character(c14raw$PeriodCode))%in%periodcode$Code))
{
# The following step updates the lookup table
code.tmp=data.frame(Code=unique(as.character(c14raw$PeriodCode)),stringsAsFactors = FALSE)
periodcode.tmp=left_join(code.tmp,periodcode,by="Code")
write.csv(periodcode.tmp,file="./lookup/periodcode_to_do.csv",row.names=F)
}

c14raw=left_join(x=c14raw,y=periodcode,by=c("PeriodCode"="Code"))
```



## Site Names

Site names have been romanised by first using the Nabunken Site Report Database to automatically extract the _furigana_ of site names in _kanji_, followed by a manual check for fixing potential mismatch and missing _furigana_. Romanisation has been carried out using the `kana2roma()` function from the `Nippon` package. 
```{r}
library(Nippon)
source(here('utility','siteNameTranscript.R'))
### The following block is executed only in the very first iteration:
#siteList = unique(c14raw$SiteName)
#sitenames=siteNameTranscript(siteList)
##Export for manual editing
#sitenames$matched=(sitenames$SiteName==sitenames$MatchedName)
#sitenames$ToCheck = !sitenames$matched | is.na(sitenames$Furigana)
#sitenames=dplyr::select(sitenames,SiteName,Furigana,ToCheck)
#sitenames$Alternative=NA
#write.csv(sitenames,"./lookup/sitenamesTranscript.csv",row.names = FALSE)

###This block is from second iteration onwards
#Import manually edited file
sitenames=read.csv(here('lookup','sitenamesTranscript.csv'),stringsAsFactors = FALSE)

siteList = unique(c14raw$SiteName)

# The following step updates the lookup table in case there are some missing sites
if (!all(unique(siteList)%in%sitenames$SiteName))
{
  missingSites = unique(siteList)[which(!unique(siteList)%in%sitenames$SiteName)]
  additional_sitenames=siteNameTranscript(missingSites)
  sitenames=rbind.data.frame(sitenames,data.frame(SiteName=additional_sitenames$SiteName,
                                                      Furigana=additional_sitenames$Furigana,
                                                      ToCheck=TRUE,
                                                      Alternative=NA))
  write.csv(sitenames,here('lookup','sitenamesTranscript.csv'),row.names = FALSE)
}

# The section reads and merges the sitenames to the main data.frame
sitenames=read.csv(here('lookup','sitenamesTranscript.csv'),stringsAsFactors = FALSE,na.strings = '')

### Processing Site Type ####
sitenames$SiteType=NA
# 遺跡群
sitenames$SiteType[grep('いせきぐん',sitenames$Furigana)]='Site Cluster'
# 遺跡
sitenames$SiteType[which(is.na(sitenames$SiteType)&grepl('いせき',sitenames$Furigana))]='Site'
sitenames$SiteType[which(is.na(sitenames$SiteType)&grepl('遺跡',sitenames$SiteName))]='Site'

# 貝塚
sitenames$SiteType[grep('かいづか',sitenames$Furigana)]='Shell Midden'
sitenames$SiteType[grep('貝塚',sitenames$SiteName)]='Shell Midden'

# 窯跡群
sitenames$SiteType[grep('窯跡群',sitenames$SiteName)]='Kiln Cluster'
# 窯跡
sitenames$SiteType[which(is.na(sitenames$SiteType)&grepl('窯跡',sitenames$SiteName))]='Kiln'
sitenames$SiteType[grep('窯跡群',sitenames$SiteName)]='Kiln'
sitenames$SiteType[grep('窯',sitenames$SiteName)]='Kiln'

# 館跡
sitenames$SiteType[grep('館跡',sitenames$SiteName)]='Fort'
# 城跡
sitenames$SiteType[grep('城跡',sitenames$SiteName)]='Castle'
# 古墳
sitenames$SiteType[grep('古墳',sitenames$SiteName)]='Kofun'
# 洞窟
sitenames$SiteType[grep('洞窟',sitenames$SiteName)]='Cave'
sitenames$SiteType[grep('洞穴',sitenames$SiteName)]='Cave'
# 城
sitenames$SiteType[grep('城',sitenames$SiteName)]='Castle'
# 横穴
sitenames$SiteType[grep('横穴',sitenames$SiteName)]='Side-Hole Burial'
# 水田址
sitenames$SiteType[grep('水田址',sitenames$SiteName)]='Paddy Field'
# 寺跡
sitenames$SiteType[grep('寺跡',sitenames$SiteName)]='Temple/Shrine'
# 列石
sitenames$SiteType[grep('列石',sitenames$SiteName)]='Standing Stones'
# Everything Else
sitenames$SiteType[which(is.na(sitenames$SiteType))]='Other'

# Romanise ####
sitenames$Romanised = kana2roma(sitenames$Furigana)
sitenames$Romanised = stringr::str_to_title(sitenames$Romanised)
#Merge Back to Main DB
c14raw=left_join(x=c14raw,y=sitenames,by="SiteName")


# Report any case of sitenames with suggested correction
if (any(!is.na(c14raw$Alternative)))
{
 fix.names=subset(c14raw,!is.na(Alternative))
 write.csv(fix.names,file=here('reports','sitenames_issues.csv'))

}



```


## Assign retain to FALSE for impossible dates and NA dates
```{r}
c14raw$retain[which(c14raw$CRA>55000|c14raw$CRA<0)]=FALSE
c14raw$retain[which(is.na(c14raw$CRA)|is.na(c14raw$CRAError))]=FALSE
c14raw$retain[which(c14raw$CRAError>500)]=FALSE
```

## Assign retain to FALSE for LabCodes with '不明' or ?
```{r}
c14raw$retain[c14raw$LabCode=='不明']=FALSE
c14raw$retain[grep('\\？',c14raw$LabCode)]=FALSE
c14raw$retain[grep('\\?',c14raw$LabCode)]=FALSE
```

## Eliminate Any Duplicates
```{r}
# All Duplicates
c14raw = unique(c14raw)
# Duplicated LabCode
tmp = subset(c14raw,retain==TRUE)
dupllab=names(table(tmp$LabCode))[which(table(tmp$LabCode)>1)]
## Manual Fix ###
c14raw[which(c14raw$LabCode%in%c(dupllab))[c(1,3,5)],'retain']=FALSE
```


## Convert CRA into numeric

```{r}
c14raw$CRA=as.numeric(c14raw$CRA)
c14raw$CRAError=as.numeric(c14raw$CRAError)
c14raw$UnroundedCRA=as.numeric(c14raw$UnroundedCRA)
c14raw$UnroundedCRAError=as.numeric(c14raw$UnroundedCRAError)

c14raw$retain[which(c14raw$CRA<0)]=FALSE
c14raw$retain[which(c14raw$CRA>55000)]=FALSE
c14raw$retain[which(c14raw$CRAError<0)]=FALSE

```

## Store Output 
```{r}
# For Analyses
write.csv(c14raw,file='japanc14db_v04.2(210715).csv')
c14db=c14raw
save(c14db,file='japanc14db_v04.2(210715).RData')

c14db_fortrans = subset(c14db,retain==TRUE)

# For Public Release
c14translation = data.frame(LabCode=c14db_fortrans$LabCode,
                            SiteName=c14db_fortrans$Romanised,
                            SiteNameJP=c14db_fortrans$SiteName,
                            SiteType = c14db_fortrans$SiteType,
                            Prefecture=c14db_fortrans$PrefectureNameEn,
                            Latitude=c14db_fortrans$Latitude,
                            Longitude=c14db_fortrans$Longitude,
                            MaterialDetails=c14db_fortrans$Material_Details2,
                            MaterialClassification_lv1=c14db_fortrans$Material,
                            MaterialClassification_lv2=c14db_fortrans$Material_Details,
                            Period=c14db_fortrans$PeriodCodeEn,
                            DatingMethod=c14db_fortrans$Method_En,
                            C14Age=as.numeric(c14db_fortrans$CRA),
                            C14AgeError=as.numeric(c14db_fortrans$CRAError),
                            UnroundedC14Age=as.numeric(c14db_fortrans$UnroundedCRA),
                            UnroundedC14AgeError=as.numeric(c14db_fortrans$UnroundedCRAError),
                            Delta13C_AMS=c14db_fortrans$Delta13C,
                            Delta13C_AMSErr=c14db_fortrans$Delta13CError,
                            Delta13C_IRMS=c14db_fortrans$Delta13CIRMS,
                            OriginalReference=c14db_fortrans$Reference)

save(c14translation,file='japanc14db_translation_v04.2(210715).RData')
write.csv(c14translation,file='japanc14db_translation_v04.2(210715).csv')
```
